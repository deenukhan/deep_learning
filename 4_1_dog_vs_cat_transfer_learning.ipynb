{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.1 dog_vs_cat_transfer_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeL54zZPXpSI1dmvQ6t3uV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deenukhan/deep_learning/blob/main/4_1_dog_vs_cat_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap6rG_AcPzrj"
      },
      "source": [
        "# This Notebook is inspired by \n",
        "# https://keras.io/guides/transfer_learning/\n",
        "# https://www.coursera.org/learn/convolutional-neural-networks-tensorflow/home/welcome\n",
        "\n",
        "# In this Notebook we're going to try few State of The Computer Vision Model, Like Inception V3, Xception, VGG and more.\n",
        "# We will also be trying several Transfer learning techniques, like, using Transfer Learning for prediction, feature extraction \n",
        "# and  Fineturning the Transfer learning model according to our data\n",
        "# So Let's Get Started"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Qf0Kq2K5Yh"
      },
      "source": [
        "## **Transfer Learning** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfL4PBL7Pkxa"
      },
      "source": [
        "The typical transfer-learning workflow\n",
        "This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
        "\n",
        "1. Instantiate a base model and load pre-trained weights into it.\n",
        "2. Freeze all layers in the base model by setting trainable = False.\n",
        "3. Create a new model on top of the output of one (or several) layers from the base model.\n",
        "4. Train your new model on your new dataset.\n",
        "\n",
        "Note that an alternative, more lightweight workflow could also be:\n",
        "\n",
        "1. Instantiate a base model and load pre-trained weights into it.\n",
        "2. Run your new dataset through it and record the output of one (or several) layers from the base model. This is called feature extraction.\n",
        "3. Use that output as input data for a new, smaller model.\n",
        "\n",
        "A key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it's a lot faster & cheaper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu_Y_9VN5MGZ"
      },
      "source": [
        "# Import Some Libraries\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Flatten"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lrnbs3LU5Xn9",
        "outputId": "10d894b4-f76c-4364-93e3-97900125cece"
      },
      "source": [
        "# We will be using dog_vs_cat dataset for this Notebook\n",
        "# So Let's download the dataset and extract it\n",
        "dog_vs_cat_url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "keras.utils.get_file('dog_vs_cat.zip', dog_vs_cat_url, cache_dir = '/content/')\n",
        "\n",
        "zipfile.ZipFile('/content/datasets/dog_vs_cat.zip', 'r').extractall('/content/datasets/')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/datasets/dog_vs_cat.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOZy7u8oIGo2"
      },
      "source": [
        "# # Let's Check the Filenames \n",
        "# main_dir = '/content/datasets/cats_and_dogs_filtered'\n",
        "\n",
        "training_cats  = os.listdir('/content/datasets/cats_and_dogs_filtered/train/cats')\n",
        "training_dogs  = os.listdir('/content/datasets/cats_and_dogs_filtered/train/dogs')\n",
        "val_cats  = os.listdir('/content/datasets/cats_and_dogs_filtered/validation/cats')\n",
        "val_dogs  = os.listdir('/content/datasets/cats_and_dogs_filtered/validation/dogs')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-NmihNbIXQY",
        "outputId": "987a01e0-5c85-4d94-9abe-c257536312c0"
      },
      "source": [
        "# Printing some length and few filenames \n",
        "print(\"Training Cats Filenames : \", training_cats[:5])\n",
        "print(\"Training Dogs Filenames : \", training_dogs[:5])\n",
        "print(\"Validation Cats Filenames: \", val_cats[:5])\n",
        "print(\"Training Dogs Filenamse: \", val_dogs[:5])\n",
        "\n",
        "print(\"Total Training Dogs : \", len(training_dogs))\n",
        "print(\"Total Training Cats : \", len(training_cats))\n",
        "print(\"Total Validation Dogs : \", len(val_cats))\n",
        "print(\"Total Validation Cats : \", len(val_dogs))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Cats Filenames :  ['cat.657.jpg', 'cat.639.jpg', 'cat.776.jpg', 'cat.411.jpg', 'cat.298.jpg']\n",
            "Training Dogs Filenames :  ['dog.40.jpg', 'dog.684.jpg', 'dog.628.jpg', 'dog.320.jpg', 'dog.618.jpg']\n",
            "Validation Cats Filenames:  ['cat.2498.jpg', 'cat.2117.jpg', 'cat.2465.jpg', 'cat.2122.jpg', 'cat.2040.jpg']\n",
            "Training Dogs Filenamse:  ['dog.2467.jpg', 'dog.2190.jpg', 'dog.2453.jpg', 'dog.2129.jpg', 'dog.2002.jpg']\n",
            "Total Training Dogs :  1000\n",
            "Total Training Cats :  1000\n",
            "Total Validation Dogs :  500\n",
            "Total Validation Cats :  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO-qwcjLIjgu",
        "outputId": "2f2f99d7-4984-492b-9dcc-cee4b73dae49"
      },
      "source": [
        "# Now Let's create the Pipeline for our Model, we are using ImageDataGenerator for our Model\n",
        "# We are also using some Data Augmentation like flip, shear, rotation etc.\n",
        "img_train_gen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                rescale = 1./255,\n",
        "                horizontal_flip = True,\n",
        "                width_shift_range = 0.2,\n",
        "                vertical_flip = True,\n",
        "                shear_range = 0.2,\n",
        "                rotation_range = 40,\n",
        "                zoom_range = 0.2 \n",
        ")\n",
        "img_val_gen = keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "img_train_gen = img_train_gen.flow_from_directory(\n",
        "                \"/content/datasets/cats_and_dogs_filtered/train\",\n",
        "                target_size = (150, 150),\n",
        "                class_mode = 'binary', batch_size = 32,\n",
        "                shuffle = True\n",
        "                )\n",
        "img_val_gen = img_val_gen.flow_from_directory(\n",
        "                \"/content/datasets/cats_and_dogs_filtered/validation\",\n",
        "                target_size = (150, 150),\n",
        "                class_mode = 'binary', batch_size = 32,\n",
        "                shuffle = True\n",
        "                )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpg0EWSzReEO"
      },
      "source": [
        "## **Inception V3 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9xr6psCJHUn"
      },
      "source": [
        "# You can read more about this model on this link https://keras.io/api/applications/inceptionv3/ \n",
        "# First We need to Load the Model \n",
        "\n",
        "base_model = keras.applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# for layers in base_model.layers:\n",
        "#     layers.trainable = False"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEYhgcg0L3nt"
      },
      "source": [
        "# Here We are using only one Dense Layer after the InceptionV3 Model to Just to keep thing Simple\n",
        "inputs = Input(shape=(150,150,3))\n",
        "x = base_model(inputs)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(1, activation = 'sigmoid')(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsuY7U3lNlc7",
        "outputId": "8869f679-68db-486b-ecdf-966c9f999fef"
      },
      "source": [
        "# Let's Compile and Train the Model\n",
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(img_train_gen, epochs = 10, validation_data=img_val_gen)\n",
        "\n",
        "# Here We can see that Our Model has got 95 Percent Accuracy In Just 10 epochs\n",
        "# That is Amazing, We got around 70 Percent accuracy when we built our model earlier from scratch in 10 epochs"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 61s 410ms/step - loss: 3.9260 - accuracy: 0.6583 - val_loss: 0.1692 - val_accuracy: 0.9390\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 22s 356ms/step - loss: 0.3784 - accuracy: 0.8591 - val_loss: 0.1638 - val_accuracy: 0.9370\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 23s 363ms/step - loss: 0.3282 - accuracy: 0.8630 - val_loss: 0.1177 - val_accuracy: 0.9560\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 22s 353ms/step - loss: 0.2507 - accuracy: 0.8974 - val_loss: 0.1764 - val_accuracy: 0.9300\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 23s 358ms/step - loss: 0.2928 - accuracy: 0.8851 - val_loss: 0.1645 - val_accuracy: 0.9370\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 22s 351ms/step - loss: 0.2535 - accuracy: 0.8878 - val_loss: 0.1131 - val_accuracy: 0.9570\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 22s 356ms/step - loss: 0.2438 - accuracy: 0.9084 - val_loss: 0.1232 - val_accuracy: 0.9540\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 22s 350ms/step - loss: 0.2361 - accuracy: 0.9018 - val_loss: 0.1604 - val_accuracy: 0.9470\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 22s 348ms/step - loss: 0.2719 - accuracy: 0.8989 - val_loss: 0.1308 - val_accuracy: 0.9450\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 23s 363ms/step - loss: 0.2691 - accuracy: 0.8905 - val_loss: 0.1316 - val_accuracy: 0.9550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7effff966f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNZVizFFS2lE"
      },
      "source": [
        "## **Xception Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXUEISSPNvJK"
      },
      "source": [
        "# And with Just a Single Word Change We can implement Another Model That is Amazing\n",
        "# You can read more about this model on this link https://keras.io/api/applications/xception/ \n",
        "# First We need to Load the Model \n",
        "\n",
        "base_model = keras.applications.Xception(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# for layers in base_model.layers:\n",
        "#     layers.trainable = False"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JVxioHbTK3S",
        "outputId": "7bd1cef5-7f7b-45e1-e1c7-812b5f04ba56"
      },
      "source": [
        "# Here We are using only one Dense Layer after the Xception Model to Just to keep thing Simple\n",
        "keras.backend.clear_session()\n",
        "\n",
        "inputs = Input(shape=(150,150,3))\n",
        "x = base_model(inputs)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(1, activation = 'sigmoid')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Let's Compile and Train the Model\n",
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(img_train_gen, epochs = 10, validation_data=img_val_gen)\n",
        "\n",
        "# It Feels Like We don't even need to Train our Dense layer more than 1 epoch, because we are getting around 95% accuracy Just like thatn\n",
        "# And This model and Given us more than 96% Accuracy on Validation Dataset, That is just great!!"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 30s 408ms/step - loss: 2.5784 - accuracy: 0.7288 - val_loss: 0.1757 - val_accuracy: 0.9410\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.4018 - accuracy: 0.8581 - val_loss: 0.1523 - val_accuracy: 0.9500\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.2895 - accuracy: 0.8830 - val_loss: 0.1648 - val_accuracy: 0.9430\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.2515 - accuracy: 0.8935 - val_loss: 0.1257 - val_accuracy: 0.9550\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.2519 - accuracy: 0.9008 - val_loss: 0.1560 - val_accuracy: 0.9450\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.2596 - accuracy: 0.8993 - val_loss: 0.1032 - val_accuracy: 0.9630\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.2426 - accuracy: 0.8892 - val_loss: 0.1247 - val_accuracy: 0.9480\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.2307 - accuracy: 0.9002 - val_loss: 0.1186 - val_accuracy: 0.9450\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.1931 - accuracy: 0.9212 - val_loss: 0.1452 - val_accuracy: 0.9320\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.2345 - accuracy: 0.9010 - val_loss: 0.1145 - val_accuracy: 0.9570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efffe01fdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrgRcMt-UYtV"
      },
      "source": [
        "## **VGG16 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmGJk9K5Ti11"
      },
      "source": [
        "# And with Just a Single Word Change We can implement Another Model That is Amazing\n",
        "# You can read more about this model on this link https://keras.io/api/applications/vgg/#vgg16-function \n",
        "# First We need to Load the Model \n",
        "\n",
        "base_model = keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# for layers in base_model.layers:\n",
        "#     layers.trainable = False"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fxOz1slUjxa",
        "outputId": "fc1cc4e5-7d27-4422-efd7-59689006c283"
      },
      "source": [
        "# Here We are using only one Dense Layer after the VGG16 Model to Just to keep thing Simple\n",
        "keras.backend.clear_session()\n",
        "\n",
        "inputs = Input(shape=(150,150,3))\n",
        "x = base_model(inputs, training= False)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(1, activation = 'sigmoid')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Let's Compile and Train the Model\n",
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(img_train_gen, epochs = 10, validation_data=img_val_gen)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.7899 - accuracy: 0.6451 - val_loss: 0.3440 - val_accuracy: 0.8460\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.4654 - accuracy: 0.7743 - val_loss: 0.4858 - val_accuracy: 0.7630\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.4272 - accuracy: 0.7901 - val_loss: 0.3104 - val_accuracy: 0.8650\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.3784 - accuracy: 0.8349 - val_loss: 0.3184 - val_accuracy: 0.8610\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 23s 370ms/step - loss: 0.3651 - accuracy: 0.8251 - val_loss: 0.3254 - val_accuracy: 0.8630\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 23s 371ms/step - loss: 0.3799 - accuracy: 0.8303 - val_loss: 0.3003 - val_accuracy: 0.8620\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 23s 370ms/step - loss: 0.3483 - accuracy: 0.8375 - val_loss: 0.3076 - val_accuracy: 0.8670\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.3551 - accuracy: 0.8437 - val_loss: 0.3002 - val_accuracy: 0.8670\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.3173 - accuracy: 0.8539 - val_loss: 0.3189 - val_accuracy: 0.8650\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 23s 365ms/step - loss: 0.3583 - accuracy: 0.8423 - val_loss: 0.3065 - val_accuracy: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7eff906bd2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHkB0Xq6WHlD"
      },
      "source": [
        "## **Fine Tune VGG-16 Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np-_HlyPWYfE"
      },
      "source": [
        "Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\n",
        "\n",
        "This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind.\n",
        "\n",
        "Finally, let's unfreeze the base model and train the entire model end-to-end with a low learning rate.\n",
        "\n",
        "Importantly, although the base model becomes trainable, it is still running in inference mode since we passed training=False when calling it when we built the model. This means that the batch normalization layers inside won't update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flNEnVFJUlv3"
      },
      "source": [
        "base_model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUsMUQjZWmU8",
        "outputId": "769916bf-4b90-4ff8-d2dd-bf7a4210e823"
      },
      "source": [
        "# Let's Compile and Train the Model\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(img_train_gen, epochs = 10, validation_data=img_val_gen)\n",
        "\n",
        "# Here We can see that accuracy has increased a little bit, but we can train it for more epoch and change the paramaters for better results"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.3300 - accuracy: 0.8593 - val_loss: 0.3100 - val_accuracy: 0.8730\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 23s 365ms/step - loss: 0.3366 - accuracy: 0.8535 - val_loss: 0.3119 - val_accuracy: 0.8740\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 23s 364ms/step - loss: 0.3274 - accuracy: 0.8538 - val_loss: 0.3102 - val_accuracy: 0.8760\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 23s 362ms/step - loss: 0.3233 - accuracy: 0.8582 - val_loss: 0.3091 - val_accuracy: 0.8750\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 23s 365ms/step - loss: 0.3230 - accuracy: 0.8521 - val_loss: 0.3072 - val_accuracy: 0.8760\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 23s 363ms/step - loss: 0.3219 - accuracy: 0.8686 - val_loss: 0.3096 - val_accuracy: 0.8760\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 23s 362ms/step - loss: 0.3342 - accuracy: 0.8560 - val_loss: 0.3078 - val_accuracy: 0.8780\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 23s 365ms/step - loss: 0.3026 - accuracy: 0.8728 - val_loss: 0.3090 - val_accuracy: 0.8760\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 23s 366ms/step - loss: 0.3206 - accuracy: 0.8624 - val_loss: 0.3049 - val_accuracy: 0.8780\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 23s 365ms/step - loss: 0.3299 - accuracy: 0.8533 - val_loss: 0.3067 - val_accuracy: 0.8780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7eff90608a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}