{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Loading and Pre-processing in TF_KERAS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVa2+j4aEFhPRay2VB71RD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deenukhan/deep_learning/blob/main/3_Loading_and_Pre_processing_in_TF_KERAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ZuObjGIavF"
      },
      "source": [
        "# This Notebook is inspired the book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
        "\n",
        "# In this notebook we will be performing below tasks\n",
        "\n",
        "# 1. In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess \n",
        "#    it efficiently, then build and train a binary classification model containing an Embedding layer:\n",
        "\n",
        "    # Download the Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. \n",
        "    # The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and \n",
        "    # a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders \n",
        "    # (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
        "\n",
        "    # Split the test set into a validation set (15,000) and a test set (10,000).\n",
        "\n",
        "    # Use tf.data to create an efficient dataset for each set.\n",
        "\n",
        "    # Create a binary classification model, using a TextVectorization layer to preprocess each review. \n",
        "    # If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: \n",
        "    # you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation \n",
        "    # with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method.\n",
        "\n",
        "    # Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words. \n",
        "    # This rescaled mean embedding can then be passed to the rest of your model.\n",
        "\n",
        "    # Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.\n",
        "\n",
        "    # Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\")."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZh4EOMPTq0j"
      },
      "source": [
        "## **Second Task :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRoweYQLlK4"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHFG4sTlMAtH"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(33)\n",
        "tf.random.set_seed(44)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnfn15rzQx3L",
        "outputId": "c740d67f-6c8e-41f7-fc02-dbd446a25840"
      },
      "source": [
        "# First Let's Download the dataset and complete the first step\n",
        "    # Download the Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. \n",
        "    # The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and \n",
        "    # a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders \n",
        "    # (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True, cache_dir='/content/' )\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "path\n",
        "\n",
        "\n",
        "# import tensorflow_datasets as tfds\n",
        "\n",
        "# datasets = tfds.load(name=\"imdb_reviews\")\n",
        "# train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datasets/aclImdb')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ORguOTNVG_R",
        "outputId": "1b76d0ef-bab8-4872-ec92-968a26cb4d10"
      },
      "source": [
        "# # Below Code is just for showing the Heirarchial Structure of the Root Folder of dataset\n",
        "# # This can be analysed at OS level\n",
        "for name, subdirs, files in os.walk(path):\n",
        "    indent = len(Path(name).parts) - len(path.parts)\n",
        "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
        "    for index, filename in enumerate(sorted(files)):\n",
        "        if index == 3:\n",
        "            print(\"    \" * (indent + 1) + \"...\")\n",
        "            break\n",
        "        print(\"    \" * (indent + 1) + filename)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb/\n",
            "    README\n",
            "    imdb.vocab\n",
            "    imdbEr.txt\n",
            "    train/\n",
            "        labeledBow.feat\n",
            "        unsupBow.feat\n",
            "        urls_neg.txt\n",
            "        ...\n",
            "        neg/\n",
            "            0_3.txt\n",
            "            10000_4.txt\n",
            "            10001_4.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_9.txt\n",
            "            10000_8.txt\n",
            "            10001_10.txt\n",
            "            ...\n",
            "        unsup/\n",
            "            0_0.txt\n",
            "            10000_0.txt\n",
            "            10001_0.txt\n",
            "            ...\n",
            "    test/\n",
            "        labeledBow.feat\n",
            "        urls_neg.txt\n",
            "        urls_pos.txt\n",
            "        neg/\n",
            "            0_2.txt\n",
            "            10000_4.txt\n",
            "            10001_1.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_10.txt\n",
            "            10000_7.txt\n",
            "            10001_9.txt\n",
            "            ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtiSSXnLY-1n",
        "outputId": "c725d70e-be1e-45c8-930c-bfd42ff3a290"
      },
      "source": [
        "path"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datasets/aclImdb')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_JeT-ZPXz2_",
        "outputId": "54ce8eb5-6efb-4062-ac8d-f741c73ec83b"
      },
      "source": [
        "# # Let's Find out the total reviews file for both training and testing data\n",
        "def review_paths(dirpath):\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "train_pos = review_paths(path / 'train' / 'pos')\n",
        "train_neg = review_paths(path / 'train' / 'neg')\n",
        "test_pos = review_paths(path / 'test' / 'pos')\n",
        "test_neg = review_paths(path / 'test' / 'neg')\n",
        "\n",
        "print(\"Training Positive Reviews Count : \", len(train_pos))\n",
        "print(\"Training Negative Reviews Count : \", len(train_neg))\n",
        "print(\"Testing Positive Reviews Count : \", len(test_pos))\n",
        "print(\"Testing Negative Reviews Count : \", len(test_neg))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Positive Reviews Count :  12500\n",
            "Training Negative Reviews Count :  12500\n",
            "Testing Positive Reviews Count :  12500\n",
            "Testing Negative Reviews Count :  12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us7rcfseZTRZ"
      },
      "source": [
        "# Let's Split the test set into a validation set and a test set.\n",
        "valid_pos = test_pos[:5000]\n",
        "valid_neg = test_neg[:5000]\n",
        "test_pos = test_pos[5000:]\n",
        "test_neg = test_neg[5000:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELmaDOF_a6Je",
        "outputId": "3727c13d-3691-4163-fed4-b3af5def92c2"
      },
      "source": [
        "# Let's Print out the total Count of Training, Testing and Validation data set\n",
        "print(\"Training Positive Reviews Count : \", len(train_pos))\n",
        "print(\"Training Negative Reviews Count : \", len(train_neg))\n",
        "print(\"Testing Positive Reviews Count : \", len(test_pos))\n",
        "print(\"Testing Negative Reviews Count : \", len(test_neg))\n",
        "print(\"Validation Postiive Reviews Count : \", len(valid_pos))\n",
        "print(\"Validation Negative Reviews Count : \", len(valid_neg))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Positive Reviews Count :  12500\n",
            "Training Negative Reviews Count :  12500\n",
            "Testing Positive Reviews Count :  7500\n",
            "Testing Negative Reviews Count :  7500\n",
            "Validation Postiive Reviews Count :  5000\n",
            "Validation Negative Reviews Count :  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAYh53RJbFw-"
      },
      "source": [
        "# Let's Use tf.data to create an efficient dataset for each set.\n",
        "# Since the dataset fits in memory, we can just load all the data using pure Python code and use tf.data.Dataset.from_tensor_slices():\n",
        "\n",
        "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
        "        for filepath in filepaths:\n",
        "            with open(filepath) as review_file:\n",
        "                reviews.append(review_file.read())\n",
        "            labels.append(label)\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.constant(reviews), tf.constant(labels))\n",
        "        )\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPLIMIuulcKG",
        "outputId": "c056454e-3652-496e-b31e-072cd04389cc"
      },
      "source": [
        "#Let's Print few of the reviews\n",
        "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
        "    print(X)\n",
        "    print(y)\n",
        "    print()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b\"Strange, almost all reviewers are highly positive about this movie. Is it because it's from 1975 and has Chamberlain and Curtis in it and therefore forgive the by times very bad acting and childish ways of storytelling? <br /><br />Maybe it's because some people get sentimental about this film because they have read the book? (I have not read the book, but I don't think that's a problem, film makers never presume that the viewers have read the book). <br /><br />Or is it because I am subconsciously irritated about the fact that English-speaking actors try to behave as their French counterparts?\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n",
            "tf.Tensor(b'For anyone who has seen and fallen in love with the stage musical A CHORUS LINE, the movie is a shoddy substitute. Not only are songs cut, but unnecessary plot twists added, new dance sequences choreographed, and, let\\'s face it, Richard Attenborough just doesn\\'t know how to film dancers.<br /><br />Onstage, Michael Bennett\\'s A CHORUS LINE was just that: Michael Bennett. His idea, his choreography, his direction, his gift to Broadway and the rest of the world. It was two hours of hard-hitting, in-your-face realism that really made you feel for these \"boys\" and \"girls.\" The movie, however, lacks empathy and depth: the actors look like they are auditioning for A CHORUS LINE rather than actually auditioning. Every move, every line of dialogue seems so weighted and planned; Michael Douglas, especially, as Zach is too in control for us to believe that he is this extraordinarily bitchy choreographer. Even when he throws his temper tantrums, you never quite believe him because every gesture, every accented word, every nuance is so obviously rehearsed. And as for him not dancing: Kevin Kline auditioned for the role of Zach on Broadway. Michael Bennett loved his reading, but Kline couldn\\'t dance and ultimately lost the part. How I wish they had done the same for Douglas! A CHORUS LINE is supposed to be a show about nobodies, and aside from a few recognizable faces (Vicki Frederick, who played Cassie on Broadway, as Sheila and Khandi Alexander, of TV\\'s NewsRadio, as one of the many auditioning dancers) you\\'re not supposed to KNOW any of these people. Because you DO know these people. Having a star in any of the roles is a terrible decision: when you focus on Michael Douglas and his ranting instead of on the girls and boys on the line and their stories, you lose something.<br /><br />It is truly unfortunate that the best sequence in the show (Montage: Hello Twelve, Hello Thirteen, Hello Love) is cut drastically to make way for a terrible new song entitled \"Surprise, Surprise\" that surprisingly received a nomination at the Oscars. Cassie\\'s \"mirror dance\" has a new song and tragically boring choreography -- one wonders why they bothered to shoot a movie version at all if they were going to mess with a working formula this much.<br /><br />For fans of musical theatre and those who enjoyed the stage version, this movie is a sad mockery of everything they cherished and loved. For those who never got to see the original production, either on Broadway or on tour, this movie is the only reference they will have to go by. And they\\'ll have to wonder just how it got to be the longest-running musical in Broadway history -- until a little show called CATS overtook it in the late 1990\\'s. But THAT is a different story, and don\\'t even get me started there.', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n",
            "tf.Tensor(b\"Probably the only thing that got the movie up to a four for me is the fact that I love Peter Falk. One of the world's great portrayers of bumbling incompetence . . . and yet he is one of the only anchors that prevents this from being a chaotic disaster. As Pops Romano, he provides a respectable mix of gangster charm and straight man to Chris Kattan's manic foolishness. Respectable performances are also offered by Richard Roundtree as the harried boss, Vinessa Shaw as a talented female FBI agent bouncing her head off a glass ceiling and Fred Ward as Falk's advisor and Benedict Arnold.<br /><br />The plot concept actually has some wonderful possibilities and, in the hands of a young Steve Martin or Chevy Chase, could have proved a great comedic vehicle. Kattan, who seems to idolize Ernest or Pee Wee Herman, just provides a muddled mess. Sadly, Peter Berg and Chris Penn, who portray his misfit brothers, both fall far short of their proven capability.<br /><br />There are some very funny scenes, but they are far too few and separated by way too many boring ones. What I truly miss here is what always attracted me to the Leslie Neilsen movies. There is no 'second level' of wit riding over the slapstick. No cultural references that only the adults get. . no double entendre. . it is just silly.<br /><br />And, by the way, this doesn't all mean that I am recommending it for your 9-year-old, because hopefully they have better taste and less fascination with some of their body parts and their functions.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYMVg58Jln6w"
      },
      "source": [
        "# Let's make the dataset \n",
        "batch_size = 32\n",
        "\n",
        "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
        "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
        "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEuCVmbnxKU"
      },
      "source": [
        "Now Let's, Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method._\n",
        "\n",
        "Let's first write a function to preprocess the reviews, cropping them to 300 characters, converting them to lower case, then replacing <br /> and all non-letter characters to spaces, splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly n_words tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7jHrXvynxv4",
        "outputId": "1bbb588d-a1a6-4ab8-a6ae-a21b11e211d2"
      },
      "source": [
        "def preprocess(X_batch, n_words=50):\n",
        "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
        "    Z = tf.strings.substr(X_batch, 0, 300)\n",
        "    Z = tf.strings.lower(Z)\n",
        "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
        "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
        "    Z = tf.strings.split(Z)\n",
        "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
        "\n",
        "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
        "preprocess(X_example)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
              "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
              "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
              "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>']], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9KuxuBspYiH"
      },
      "source": [
        "Now let's write a second utility function that will take a data sample with the same format as the output of the preprocess() function, and will output the list of the top max_size most frequent words, ensuring that the padding token is first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kiElIuypJ25",
        "outputId": "b8d8a3bf-8525-4469-acfe-cc44d0736c5b"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_vocabulary(data_sample, max_size=1000):\n",
        "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
        "    counter = Counter()\n",
        "    for words in preprocessed_reviews:\n",
        "        for word in words:\n",
        "            if word != b\"<pad>\":\n",
        "                counter[word] += 1\n",
        "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
        "\n",
        "get_vocabulary(X_example)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'<pad>',\n",
              " b'it',\n",
              " b'great',\n",
              " b's',\n",
              " b'a',\n",
              " b'movie',\n",
              " b'i',\n",
              " b'loved',\n",
              " b'was',\n",
              " b'terrible',\n",
              " b'run',\n",
              " b'away']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKS-E2D_p7He"
      },
      "source": [
        "Now we are ready to create the TextVectorization layer. Its constructor just saves the hyperparameters (max_vocabulary_size and n_oov_buckets). The adapt() method computes the vocabulary using the get_vocabulary() function, then it builds a StaticVocabularyTable (see Chapter 16 for more details). The call() method preprocesses the reviews to get a padded list of words for each review, then it uses the StaticVocabularyTable to lookup the index of each word in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZv6PnoQpXwo"
      },
      "source": [
        "class TextVectorization(keras.layers.Layer):\n",
        "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.max_vocabulary_size = max_vocabulary_size\n",
        "        self.n_oov_buckets = n_oov_buckets\n",
        "\n",
        "    def adapt(self, data_sample):\n",
        "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
        "        words = tf.constant(self.vocab)\n",
        "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
        "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        preprocessed_inputs = preprocess(inputs)\n",
        "        return self.table.lookup(preprocessed_inputs)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrBHFpZdp_GC",
        "outputId": "8cd6786b-bd4b-4a12-fd0a-b5b3df38cc39"
      },
      "source": [
        "# Let's try it on our small X_example we defined earlier:\n",
        "text_vectorization = TextVectorization()\n",
        "\n",
        "text_vectorization.adapt(X_example)\n",
        "text_vectorization(X_example)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
              "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0],\n",
              "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWS-EBBUqScr"
      },
      "source": [
        "Looks good! As you can see, each review was cleaned up and tokenized, then each word was encoded as its index in the vocabulary (all the 0s correspond to the <pad> tokens).\n",
        "\n",
        "Now let's create another TextVectorization layer and let's adapt it to the full IMDB training set (if the training set did not fit in RAM, we could just use a smaller sample of the training set by calling train_set.take(500)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7kiUMTSqB1m"
      },
      "source": [
        "max_vocabulary_size = 100\n",
        "n_oov_buckets = 100\n",
        "\n",
        "sample_review_batches = train_set.map(lambda review, label: review)\n",
        "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
        "                                axis=0)\n",
        "\n",
        "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
        "                                       input_shape=[])\n",
        "text_vectorization.adapt(sample_reviews)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-k_ctXjqUxc",
        "outputId": "ee340d19-08d6-4066-e0cc-206f27cd4b1d"
      },
      "source": [
        "# Let's run it on the same X_example, just to make sure the word IDs are larger now, since the vocabulary is bigger:\n",
        "\n",
        "text_vectorization(X_example)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
              "array([[  9,  14,   2,  64,  64,  12,   5, 139,   9,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  9,  13, 189, 177, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6wQJLVTtiHz",
        "outputId": "705bff44-56ba-46c5-a47f-b340992907d7"
      },
      "source": [
        "# Good! Now let's take a look at the first 10 words in the vocabulary:\n",
        "text_vectorization.vocab[:10]\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F8X93httqzc"
      },
      "source": [
        "These are the most common words in the reviews.\n",
        "\n",
        "Now to build our model we will need to encode all these word IDs somehow. One approach is to create bags of words: for each review, and for each word in the vocabulary, we count the number of occurences of that word in the review. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaAnXLlrtouU",
        "outputId": "188d2f01-7390-4743-f664-70be110672ca"
      },
      "source": [
        "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
        "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
              "array([[2., 2., 0., 1.],\n",
              "       [3., 0., 2., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l46aUeBUtwXD"
      },
      "source": [
        "The first review has 2 times the word 0, 2 times the word 1, 0 times the word 2, and 1 time the word 3, so its bag-of-words representation is [2, 2, 0, 1]. Similarly, the second review has 3 times the word 0, 0 times the word 1, and so on. Let's wrap this logic in a small custom layer, and let's test it. We'll drop the counts for the word 0, since this corresponds to the <pad> token, which we don't care about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20PxLErGtty6"
      },
      "source": [
        "class BagOfWords(keras.layers.Layer):\n",
        "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.n_tokens = n_tokens\n",
        "    def call(self, inputs):\n",
        "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
        "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMfaYCU4tyfQ",
        "outputId": "7044f764-3ada-40a9-fbe7-61b1d8c30871"
      },
      "source": [
        "# let's Test it\n",
        "bag_of_words = BagOfWords(n_tokens=4)\n",
        "bag_of_words(simple_example)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[2., 0., 1.],\n",
              "       [0., 2., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO0h_DzDt1TA"
      },
      "source": [
        "# It works fine! Now let's create another BagOfWord with the right vocabulary size for our training set:\n",
        "\n",
        "\n",
        "n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\n",
        "bag_of_words = BagOfWords(n_tokens)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pijEx7Eot8J-"
      },
      "source": [
        "We're ready to train the model!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_BCF8Fht6eB",
        "outputId": "82f7fc62-c167-4e3b-9e32-ffe223c8ffd7"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    text_vectorization,\n",
        "    bag_of_words,\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 6s 6ms/step - loss: 0.6515 - accuracy: 0.6088 - val_loss: 0.6130 - val_accuracy: 0.6596\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5963 - accuracy: 0.6726 - val_loss: 0.6105 - val_accuracy: 0.6622\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5756 - accuracy: 0.6945 - val_loss: 0.6068 - val_accuracy: 0.6640\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5475 - accuracy: 0.7184 - val_loss: 0.6145 - val_accuracy: 0.6637\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5225 - accuracy: 0.7412 - val_loss: 0.6240 - val_accuracy: 0.6558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9465a13490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1HyBMScuNLV"
      },
      "source": [
        "We get about 65% accuracy on the validation set after just the first epoch, but after that the model makes no significant progress. But it's okay as for now the point is just to perform efficient preprocessing using tf.data and Keras preprocessing layers."
      ]
    }
  ]
}